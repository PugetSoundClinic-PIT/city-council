{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from cdp_data import datasets, CDPInstances\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set randomness\n",
    "np.random.seed(60)\n",
    "\n",
    "MODEL_DIR = \"../whole-pc-section-window-classifier\"\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "# Load the model and tokenizer\n",
    "clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=MODEL_DIR,\n",
    "    tokenizer=MODEL_DIR,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "########################################################################################\n",
    "# Functions to handle segmentation evaluation\n",
    "\n",
    "def create_positions_from_indices(session_annotations: pd.DataFrame, total_transcript_length: int) -> list[int]:\n",
    "    # Convert from list of start and end indicies to \"positions\"\n",
    "    # positions format is a list of integers where each integer is an id for the unique section of the meeting\n",
    "    # i.e. [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3] has three sections\n",
    "    # First place 1's until the first start index, then 2's until the first end index,\n",
    "    # then 3's until the second start index, etc.\n",
    "\n",
    "    # Process positions\n",
    "    positions = []\n",
    "\n",
    "    # Get all the start and end indicies in a single ordered list\n",
    "    start_and_end_indicies = list(\n",
    "        session_annotations[\"period_start_sentence_index\"].values\n",
    "    ) + list(session_annotations[\"period_end_sentence_index\"].values)\n",
    "    start_and_end_indicies.sort()\n",
    "\n",
    "    # If all values are -1 then return a list of 1's for the entire length\n",
    "    if all(x == -1 for x in start_and_end_indicies):\n",
    "        return [1] * total_transcript_length\n",
    "    \n",
    "    # Add the total length of the transcript to the end\n",
    "    start_and_end_indicies.append(total_transcript_length)\n",
    "\n",
    "    # Iterate over the start and end indicies and add the positions\n",
    "    previous_boundary_index = 0\n",
    "    for section_index, sentence_index in enumerate(start_and_end_indicies, start=1):\n",
    "        length_of_section = sentence_index - previous_boundary_index\n",
    "        positions.extend([section_index] * length_of_section)\n",
    "        previous_boundary_index = sentence_index\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Load the basic dataset\n",
    "ds = datasets.get_session_dataset(\n",
    "    CDPInstances.Seattle,\n",
    "    store_transcript=True,\n",
    "    store_transcript_as_csv=True,\n",
    "    start_datetime=\"2020-01-01\",\n",
    "    end_datetime=\"2024-01-01\",\n",
    "    sample=5,\n",
    ")\n",
    "\n",
    "# Overall directory for saving\n",
    "storage_dir = Path(\"seattle-transcripts/\")\n",
    "storage_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Iter sessions \n",
    "for _, row in ds.iterrows():\n",
    "    transcript_copy_path = storage_dir / f\"{row['id']}.csv\"\n",
    "    transcript = pd.read_csv(row.transcript_as_csv_path)\n",
    "    transcript = transcript[[\"index\", \"text\"]]\n",
    "    transcript = transcript.rename(columns={\"index\": \"sentence_index\"})\n",
    "    transcript[\"session_id\"] = row[\"id\"]\n",
    "    transcript[\"council\"] = CDPInstances.Seattle\n",
    "    transcript.to_csv(transcript_copy_path, index=False)\n",
    "\n",
    "# Load the resolved annotation data\n",
    "annotations = pd.read_csv(\"training-data/whole-period-seg-seattle.csv\")\n",
    "\n",
    "def get_context_window_text(transcript, center_index) -> str:\n",
    "    return \" \".join(\n",
    "        transcript.iloc[\n",
    "            max(center_index - 1, 0):\n",
    "            min(center_index + 2, len(transcript) - 1)\n",
    "        ][\"text\"]\n",
    "    ).strip()\n",
    "\n",
    "# Iter over the annotations set, load the session transcript CSV\n",
    "# Iter over session transcript and create context windows\n",
    "# Classify the context windows\n",
    "# Mark the start and stop of comment periods in a dataframe with the same columns as the annotations\n",
    "# Construct positions strings for the original annotations and the newly classified start and stop\n",
    "# Convert to masses and then compare via seg eval\n",
    "classified_windows = []\n",
    "for _, session in tqdm(\n",
    "    annotations.sample(5).iterrows(),\n",
    "    desc=\"Processing sessions\",\n",
    "    total=5,\n",
    "):\n",
    "    # Load the session transcript csv\n",
    "    transcript = pd.read_csv(f\"seattle-transcripts/{session.session_id.strip()}.csv\")\n",
    "    transcript[\"text\"] = transcript[\"text\"].fillna(\"\")\n",
    "\n",
    "    # Construct all context windows\n",
    "    context_windows = [\n",
    "        get_context_window_text(transcript, sentence.sentence_index)\n",
    "        for _, sentence in transcript.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Classify all context windows\n",
    "    results = clf(context_windows)\n",
    "\n",
    "    # Iter over context windows\n",
    "    current_pc_start = None\n",
    "    for i, result in enumerate(results):\n",
    "        # If the result is a comment period start then update the current_pc_start\n",
    "        if result[\"label\"] == \"comment-period-start\":\n",
    "            current_pc_start = i\n",
    "        \n",
    "        # If the result is a comment period end and we have already started, then add the start and end to the classified windows\n",
    "        if result[\"label\"] == \"comment-period-end\" and current_pc_start is not None:\n",
    "            classified_windows.append({\n",
    "                \"session_id\": session.session_id,\n",
    "                \"period_start_sentence_index\": current_pc_start,\n",
    "                \"period_end_sentence_index\": i,\n",
    "            })\n",
    "            current_pc_start = None\n",
    "\n",
    "    # If we have not found any comment periods then add a single row with -1's\n",
    "    if len(classified_windows) == 0:\n",
    "        classified_windows.append({\n",
    "            \"session_id\": session.session_id,\n",
    "            \"period_start_sentence_index\": -1,\n",
    "            \"period_end_sentence_index\": -1,\n",
    "        })\n",
    "\n",
    "# Convert to dataframe\n",
    "classified_windows = pd.DataFrame(classified_windows)\n",
    "classified_windows.to_csv(\"classified-windows.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
