{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Whole Comment Seg Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a copy of the transcripts in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eva/miniforge-pypy3/envs/city-council/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching each model attached to event_ref: 100%|██████████| 200/200 [00:02<00:00, 93.23it/s] \n",
      "Fetching transcripts: 100%|██████████| 200/200 [00:01<00:00, 107.76it/s]\n",
      "Converting transcripts: 100%|██████████| 200/200 [00:00<00:00, 6387.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from cdp_data import datasets, CDPInstances\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Set randomness\n",
    "np.random.seed(60)\n",
    "\n",
    "# Get random 200 from Seattle\n",
    "ds = datasets.get_session_dataset(\n",
    "    CDPInstances.Seattle,\n",
    "    store_transcript=True,\n",
    "    store_transcript_as_csv=True,\n",
    "    start_datetime=\"2020-01-01\",\n",
    "    end_datetime=\"2024-01-01\",\n",
    "    sample=200,\n",
    ")\n",
    "\n",
    "# overall directory for saving\n",
    "storage_dir = Path(\"seattle-transcripts/\")\n",
    "storage_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# iter sessions \n",
    "for _, row in ds.iterrows():\n",
    "    # create the copy path\n",
    "    transcript_copy_path = storage_dir / f\"{row['id']}.csv\"\n",
    "\n",
    "    # read the original transcript\n",
    "    transcript = pd.read_csv(row.transcript_as_csv_path)\n",
    "\n",
    "    # keep only the index and text columns\n",
    "    transcript = transcript[[\n",
    "        \"index\",\n",
    "        \"text\",\n",
    "    ]]\n",
    "\n",
    "    # rename index to sentence_index\n",
    "    transcript = transcript.rename(columns={\"index\": \"sentence_index\"})\n",
    "\n",
    "    # add column for session id\n",
    "    transcript[\"session_id\"] = row[\"id\"]\n",
    "\n",
    "    # add column for council\n",
    "    transcript[\"council\"] = CDPInstances.Seattle\n",
    "\n",
    "    # save the modified transcript\n",
    "    transcript.to_csv(transcript_copy_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We need to convert the annotations into a set of examples ready for training, with context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>19555e51df3c</td>\n",
       "      <td>Mr. Ahn, we can continue. Okay. Four in favor,...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>ab8e206e24c6</td>\n",
       "      <td>Another vendor has developed a helmet that loc...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>ab8e206e24c6</td>\n",
       "      <td>Once you have completed your public comment we...</td>\n",
       "      <td>comment-period-start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>d13fd84e7845</td>\n",
       "      <td>And thank you, Wayne, for bringing these to us...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>03e2809fffe1</td>\n",
       "      <td>Thank you. So on slide four, the slide include...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       session_id                                               text  \\\n",
       "483  19555e51df3c  Mr. Ahn, we can continue. Okay. Four in favor,...   \n",
       "587  ab8e206e24c6  Another vendor has developed a helmet that loc...   \n",
       "585  ab8e206e24c6  Once you have completed your public comment we...   \n",
       "711  d13fd84e7845  And thank you, Wayne, for bringing these to us...   \n",
       "70   03e2809fffe1  Thank you. So on slide four, the slide include...   \n",
       "\n",
       "                    label  \n",
       "483                 other  \n",
       "587                 other  \n",
       "585  comment-period-start  \n",
       "711                 other  \n",
       "70                  other  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(60)\n",
    "\n",
    "# read the annotations\n",
    "annotations = pd.read_csv(\"training-data/whole-period-seg-seattle.csv\")\n",
    "\n",
    "# we will always take N random negative samples from the same session\n",
    "n_random_samples = 3\n",
    "\n",
    "# Create context window sets\n",
    "# single sentence means the context window is 1 (only the sentence itself)\n",
    "# three sentence means the context window is 3 (1 before and 1 after)\n",
    "# five sentence means the context window is 5 (2 before and 2 after)\n",
    "single_sentence_examples = []\n",
    "three_sentence_examples = []\n",
    "five_sentence_examples = []\n",
    "\n",
    "def get_context_windows(transcript, center_index) -> tuple[str, str, str]:\n",
    "    # calculate offsets for the three and five sentence examples\n",
    "    # if the offset would go negative, we just use 0 or len(transcript)\n",
    "    three_sentence_start_index = max(center_index - 1, 0)\n",
    "    three_sentence_end_index = min(center_index + 2, len(transcript) - 1)\n",
    "    five_sentence_start_index = max(center_index - 2, 0)\n",
    "    five_sentence_end_index = min(center_index + 3, len(transcript) - 1)\n",
    "\n",
    "    # process the single sentence example\n",
    "    single_sentence = transcript.iloc[center_index][\"text\"].strip()\n",
    "\n",
    "    # process the three sentence example\n",
    "    three_sentence = \" \".join(\n",
    "        transcript.iloc[\n",
    "            three_sentence_start_index:\n",
    "            three_sentence_end_index\n",
    "        ][\"text\"]\n",
    "    ).strip()\n",
    "\n",
    "    # process the five sentence example\n",
    "    five_sentence = \" \".join(\n",
    "        transcript.iloc[\n",
    "            five_sentence_start_index:\n",
    "            five_sentence_end_index\n",
    "        ][\"text\"]\n",
    "    ).strip()\n",
    "\n",
    "    return single_sentence, three_sentence, five_sentence\n",
    "\n",
    "# iterate over the rows of the annotations and create the context window sets\n",
    "for _, row in annotations.iterrows():\n",
    "    # load the session transcript csv\n",
    "    transcript = pd.read_csv(f\"seattle-transcripts/{row.session_id.strip()}.csv\")\n",
    "\n",
    "    # Convert rows with text as NaN to empty string\n",
    "    transcript[\"text\"] = transcript[\"text\"].fillna(\"\")\n",
    "\n",
    "    # if we have a start sentence index, add all of the context windows samples\n",
    "    if not np.isnan(row.period_start_sentence_index):\n",
    "        # get the start sentence index\n",
    "        period_start_sentence_index = int(row.period_start_sentence_index)\n",
    "\n",
    "        # get the context windows\n",
    "        single_sentence, three_sentence, five_sentence = get_context_windows(\n",
    "            transcript,\n",
    "            period_start_sentence_index,\n",
    "        )\n",
    "\n",
    "        # add all as examples\n",
    "        single_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": single_sentence,\n",
    "            \"label\": \"comment-period-start\",\n",
    "        })\n",
    "        three_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": three_sentence,\n",
    "            \"label\": \"comment-period-start\",\n",
    "        })\n",
    "        five_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": five_sentence,\n",
    "            \"label\": \"comment-period-start\",\n",
    "        })\n",
    "\n",
    "    # if we have a end sentence index, add all of the context windows samples\n",
    "    if not np.isnan(row.period_end_sentence_index):\n",
    "        # get the end sentence index\n",
    "        period_end_sentence_index = int(row.period_end_sentence_index)\n",
    "\n",
    "        # get the context windows\n",
    "        single_sentence, three_sentence, five_sentence = get_context_windows(\n",
    "            transcript,\n",
    "            period_end_sentence_index,\n",
    "        )\n",
    "\n",
    "        # add all as examples\n",
    "        single_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": single_sentence,\n",
    "            \"label\": \"comment-period-end\",\n",
    "        })\n",
    "        three_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": three_sentence,\n",
    "            \"label\": \"comment-period-end\",\n",
    "        })\n",
    "        five_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": five_sentence,\n",
    "            \"label\": \"comment-period-end\",\n",
    "        })\n",
    "    \n",
    "    # choose N random negative samples from the same session\n",
    "    # we start by finding N random negative sentence indicies to use as\n",
    "    # the center of the context windows\n",
    "    # make sure we specifically exclude the start and end sentence indices\n",
    "    valid_indices = list(range(len(transcript)))\n",
    "    if not np.isnan(row.period_start_sentence_index):\n",
    "        # remove the period start sentence index, and the two before and after\n",
    "        valid_indices = list(\n",
    "            set(valid_indices) - set(\n",
    "                range(\n",
    "                    int(row.period_start_sentence_index) - 2,\n",
    "                    int(row.period_start_sentence_index) + 3,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    if not np.isnan(row.period_end_sentence_index):\n",
    "        # remove the period end sentence index, and the two before and after\n",
    "        valid_indices = list(\n",
    "            set(valid_indices) - set(\n",
    "                range(\n",
    "                    int(row.period_end_sentence_index) - 2,\n",
    "                    int(row.period_end_sentence_index) + 3,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # choose N random negative samples\n",
    "    negative_samples = np.random.choice(valid_indices, n_random_samples, replace=False)\n",
    "\n",
    "    # process the negative samples\n",
    "    for negative_sample in negative_samples:\n",
    "        # get the context windows\n",
    "        single_sentence, three_sentence, five_sentence = get_context_windows(\n",
    "            transcript,\n",
    "            negative_sample,\n",
    "        )\n",
    "\n",
    "        # add all as examples\n",
    "        single_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": single_sentence,\n",
    "            \"label\": \"other\",\n",
    "        })\n",
    "        three_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": three_sentence,\n",
    "            \"label\": \"other\",\n",
    "        })\n",
    "        five_sentence_examples.append({\n",
    "            \"session_id\": row.session_id,\n",
    "            \"text\": five_sentence,\n",
    "            \"label\": \"other\",\n",
    "        })\n",
    "\n",
    "# create the dataframes\n",
    "single_sentence_examples = pd.DataFrame(single_sentence_examples)\n",
    "three_sentence_examples = pd.DataFrame(three_sentence_examples)\n",
    "five_sentence_examples = pd.DataFrame(five_sentence_examples)\n",
    "\n",
    "# save the dataframes\n",
    "single_sentence_examples.to_csv(\"training-data/whole-comment-seg-single-sentence-examples.csv\", index=False)\n",
    "three_sentence_examples.to_csv(\"training-data/whole-comment-seg-three-sentence-examples.csv\", index=False)\n",
    "five_sentence_examples.to_csv(\"training-data/whole-comment-seg-five-sentence-examples.csv\", index=False)\n",
    "\n",
    "three_sentence_examples.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "city-council",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
